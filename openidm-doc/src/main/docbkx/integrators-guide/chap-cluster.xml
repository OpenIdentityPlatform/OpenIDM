<?xml version="1.0" encoding="UTF-8"?>
<!--
  ! CCPL HEADER START
  !
  ! This work is licensed under the Creative Commons
  ! Attribution-NonCommercial-NoDerivs 3.0 Unported License.
  ! To view a copy of this license, visit
  ! http://creativecommons.org/licenses/by-nc-nd/3.0/
  ! or send a letter to Creative Commons, 444 Castro Street,
  ! Suite 900, Mountain View, California, 94041, USA.
  !
  ! You can also obtain a copy of the license at
  ! legal/CC-BY-NC-ND.txt.
  ! See the License for the specific language governing permissions
  ! and limitations under the License.
  !
  ! If applicable, add the following below this CCPL HEADER, with the fields
  ! enclosed by brackets "[]" replaced with your own identifying information:
  !      Portions Copyright [yyyy] [name of copyright owner]
  !
  ! CCPL HEADER END
  !
  !      Copyright 2011-2014 ForgeRock AS
  !
-->
<chapter xml:id='chap-cluster'
         xmlns='http://docbook.org/ns/docbook'
         version='5.0' xml:lang='en'
         xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
         xsi:schemaLocation='http://docbook.org/ns/docbook
         http://docbook.org/xml/5.0/xsd/docbook.xsd'
         xmlns:xlink='http://www.w3.org/1999/xlink'>
 <title>Configuring OpenIDM to Work in a Cluster</title>
 <indexterm>
  <primary>cluster management</primary>
 </indexterm>
 <indexterm>
  <primary>high availability</primary>
 </indexterm>
 <indexterm>
  <primary>failover</primary>
 </indexterm>

    <para>To ensure availability of the identity management service, you can
    deploy multiple OpenIDM instances in a cluster. In a clustered environment,
    all instances point to the same external database. The database itself
    might or might not be clustered, depending on your particular availability
    strategy.</para>

    <para>In a clustered environment, if an instance becomes unavailable or
    fails to check in with the cluster management service, another instance in
    the cluster detects this situation. If the unavailable instance was in the
    process of executing any pending jobs, the available instance attempts to
    recover these jobs.</para>

    <para>For example, if instance-1 goes down while executing a scheduled task,
    the cluster manager of instance-2 will notify instance-2's scheduler service
    that instance-1 is unavailable. The scheduler service then attempts to clean
    up any jobs that instance-1 was executing when it went down.</para>

    <para>Specific configuration changes must be made to configure multiple
    instances that point to a single database. These configuration changes
    are described in this chapter.</para>

    <section xml:id="cluster-config">
     <title>Configuring an OpenIDM Instance as Part of a Cluster</title>

        <para>Before you configure an instance to work in a cluster, make sure
        that the instance is not running. If the instance was previously started,
        delete the <filename>felix-cache</filename> folder.</para>

        <para>To configure an individual OpenIDM instance to be part of a
        clustered deployment, follow these steps.</para>

        <orderedlist>
            <listitem>
                <para>Configure OpenIDM for a MySQL repository, as described
                in <link xlink:href="install-guide#chap-repository"
                xlink:role="http://docbook.org/xlink/role/olink"><citetitle
                >Installing a Repository For Production</citetitle></link> in
                the <citetitle>Installation Guide</citetitle>.</para>

                <para>All OpenIDM instances that form part of a single cluster
                must be homogenous in terms of their repository - in other
                words, the instances must all be configured to use the same
                type of backend repository (either MySQL or MS SQL). Note that
                OrientDB is currently unsupported in production environments.</para>

                <para>Note that you only need to import the data definition
                language script for OpenIDM into MySQL once, not repeatedly for
                each OpenIDM instance.</para>
            </listitem>
            <listitem>
                <para><xref linkend="cluster-boot-config" /></para>
            </listitem>
            <listitem>
                <para><xref linkend="cluster-config-file" /></para>
            </listitem>
         <listitem>
          <para>
           If you are using scheduled tasks, make sure that schedules are
           persisted to ensure that they fire only once across the cluster. For
           more information, see the section on <link xlink:show="new"
           xlink:href="integrators-guide#persistent-schedules"
           xlink:role="http://docbook.org/xlink/role/olink"><citetitle>Persisted
           Schedules</citetitle></link>.
          </para>
         </listitem>
        </orderedlist>

        <section xml:id="cluster-boot-config">
            <title>Edit the Boot Configuration</title>

            <para>Each participating instance in a cluster must have its own
            unique node or instance ID, and must be attributed a role in the
            cluster. Specify these parameters in the
            <filename>conf/boot/boot.properties</filename> file of each
            instance.</para>

            <itemizedlist>
                <listitem>
                    <para>Specify a unique identifier for the instance. For
                    example:</para>

                    <screen>$ grep openidm.node.id /path/to/openidm/conf/boot/boot.properties
openidm.node.id=instance1</screen>

                    <para>On subsequent instances, the <literal>openidm.node.id</literal>
                    can be set to <literal>instance2</literal>, <literal>instance3</literal>,
                    and so forth. You can choose any value, as long as it is
                    unique within the cluster.</para>
                </listitem>
                <listitem>
                    <para>Specify the instance type in the cluster.</para>
                    <para>On the primary instance, add the following line to
                    the <filename>boot.properties</filename> file:</para>

                    <screen>openidm.instance.type=clustered-first</screen>

                    <para>On subsequent instances, add the following line to
                    the <filename>boot.properties</filename> file:</para>

                    <screen>openidm.instance.type=clustered-additional</screen>

                    <para>The instance type is used during the setup process.
                    When the primary node has been configured, additional nodes
                    are bootstrapped with the security settings (key store and
                    trust store) of the primary node. Once the nodes have been
                    configured, all nodes are considered equal in the cluster,
                    that is, there is no concept of a "master" node.</para>

                    <para>If no instance type is specified, the default value
                    for this property is <literal>openidm.instance.type=standalone</literal>,
                    which indicates that the instance will not be part of a
                    cluster.</para>
                </listitem>
            </itemizedlist>
        </section>

        <section xml:id="cluster-config-file">
            <title>Edit the Cluster Configuration</title>

            <para>The cluster configuration for each instance is defined in the
            file <filename>/path/to/openidm/conf/cluster.json</filename>. In
            most cases, you should be able to retain the default cluster
            configuration, which is as follows:</para>

            <programlisting language="javascript">{
"instanceId" : "&amp;{openidm.node.id}",
"instanceTimeout" : "30000",
"instanceRecoveryTimeout" : "30000",
"instanceCheckInInterval" : "5000",
"instanceCheckInOffset" : "0"
}</programlisting>

            <itemizedlist>
                <listitem>
                    <para>The ID of the instance (<literal>instanceId</literal>)
                    is set in the <filename>conf/boot/boot.properties</filename>
                    file, as described in the previous step.</para>
                </listitem>
                <listitem>
                   <para><literal>instanceTimeout</literal> specifies the length
                    of time (in milliseconds) that an instance can be "down" before the
                    instance is considered to be in recovery mode.</para>
                </listitem>
                <listitem>
                    <para><literal>instanceRecoveryTimeout</literal> specifies the
                    length of time (in milliseconds) that an instance can be in
                    recovery mode before that instance is considered to be offline.</para>
                </listitem>
                <listitem>
                    <para><literal>instanceCheckInInterval</literal> specifies the
                    frequency (in milliseconds) that this instance checks in with
                    the cluster manager to indicate that it is still online.</para>
                </listitem>
                <listitem>
                    <para><literal>instanceCheckInOffset</literal> specifies
                    an offset (in milliseconds) for the checkin timing, per
                    instance, when a number of instances in a cluster are started
                    simultaneously.</para>
                    <para>Specifying a checkin offset prevents a situation in
                    which all the instances in a cluster check in at the same
                    time, and place a strain on the cluster manager resource.</para>
                </listitem>
            </itemizedlist>

            <para>If the default cluster configuration is not suitable for your
            deployment, edit the <filename>cluster.json</filename> file for
            each instance.</para>
        </section>

    </section>

    <section xml:id="cluster-over-REST">

        <title>Managing Nodes Over REST</title>

        <para>You can manage clusters and individual nodes over the REST
        interface, at the URL
        <literal>http://localhost:8080/openidm/cluster/</literal>. The
        following sample REST commands demonstrate the cluster information
        that is available over REST.</para>

        <example>
            <title>Displaying the Nodes in the Cluster</title>

            <para>The following REST request displays the nodes configured
            in the cluster, and their status.</para>

            <screen>$ <userinput>curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster" </userinput>
             <computeroutput>{
  "results": [
    {
      "shutdown": "",
      "startup": "2013-10-28T11:48:29.026+02:00",
      "instanceId": "openidm-1",
      "state": "running"
    },
    {
      "shutdown": "",
      "startup": "2013-10-28T11:51:31.639+02:00",
      "instanceId": "openidm-2",
      "state": "running"
    }
  ]</computeroutput>
}</screen>
        </example>

        <example>
            <title>Checking the State of an Individual Node</title>

            <para>To check the status of a specific node, include its instance
            ID in the URL, for example:</para>

            <screen>$ <userinput> curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster/openidm-1"
             </userinput>
             <computeroutput>{
  "results": {
    "shutdown": "",
    "startup": "2013-10-28T11:48:29.026+02:00",
    "instanceId": "openidm-1",
    "state": "running"
  }</computeroutput>
}</screen>
        </example>
    </section>

</chapter>
