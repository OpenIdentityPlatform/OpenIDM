<?xml version="1.0" encoding="UTF-8"?>
<!--
  ! CCPL HEADER START
  !
  ! This work is licensed under the Creative Commons
  ! Attribution-NonCommercial-NoDerivs 3.0 Unported License.
  ! To view a copy of this license, visit
  ! http://creativecommons.org/licenses/by-nc-nd/3.0/
  ! or send a letter to Creative Commons, 444 Castro Street,
  ! Suite 900, Mountain View, California, 94041, USA.
  !
  ! You can also obtain a copy of the license at
  ! legal/CC-BY-NC-ND.txt.
  ! See the License for the specific language governing permissions
  ! and limitations under the License.
  !
  ! If applicable, add the following below this CCPL HEADER, with the fields
  ! enclosed by brackets "[]" replaced with your own identifying information:
  !      Portions Copyright [yyyy] [name of copyright owner]
  !
  ! CCPL HEADER END
  !
  !      Copyright 2011-2015 ForgeRock AS
  !
-->
<chapter xml:id='chap-cluster'
  xmlns='http://docbook.org/ns/docbook'
  version='5.0' xml:lang='en'
  xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'
  xsi:schemaLocation='http://docbook.org/ns/docbook
  http://docbook.org/xml/5.0/xsd/docbook.xsd'
  xmlns:xlink='http://www.w3.org/1999/xlink'>
 <title>Configuring OpenIDM to Work in a Cluster</title>
 <indexterm>
  <primary>cluster management</primary>
 </indexterm>
 <indexterm>
  <primary>high availability</primary>
 </indexterm>
 <indexterm>
  <primary>failover</primary>
 </indexterm>

 <para>
  To ensure availability of the identity management service, you can deploy
  OpenIDM on multiple instances, in a cluster. In a clustered environment, each
  OpenIDM instance must point to the same external repository.
  The database does not have to be clustered.
 </para>
 <para>
  In a clustered environment, if one OpenIDM instance in a cluster shuts down
  or fails to check in with the cluster management service, a second OpenIDM
  instance will detect the failure.
 </para>
 <para>
  For example, if an OpenIDM instance named <literal>instance1</literal> loses
  connectivity while executing a scheduled task, the cluster manager notifies
  the scheduler service that <literal>instance1</literal> is not available.
  The scheduler service then attempts to clean up any jobs that
  <literal>instance1</literal> was running at that time.
 </para>
 <para>
  This chapter describes the changes you need to make to OpenIDM to configure
  multiple instances that point to a database.
 </para>
 <para>
  The following diagram depicts a relatively simple cluster configuration.
 </para>

 <mediaobject xml:id="figure-cluster">
  <alt>A Simplified Cluster</alt>
  <imageobject>
   <imagedata fileref="images/cluster-config.png" format="PNG" />
  </imageobject>
  <textobject>
   <para>You can set up a cluster with two or more instances of OpenIDM.</para>
  </textobject>
 </mediaobject>

 <para>
  The OpenIDM cluster service is configured in three files:
  <filename>cluster.json</filename>, <filename>boot/boot.properties</filename>,
  and <filename>scheduler.json</filename>; you can find each of these files in
  the <filename>openidm/conf</filename> subdirectory. When you configure a
  cluster, you may modify these files in every OpenIDM instance in your cluster.
 </para>

 <section xml:id="cluster-config">
  <title>Configuring an OpenIDM Instance as Part of a Cluster</title>

  <para>
   Each OpenIDM instance in a cluster must be configured to use the same
   repository. As OrientDB is not supported in production environments, you
   should configure a supported repository, as described in the following
   chapter of the <citetitle>Installation Guide</citetitle>:
   <link xlink:show="new" xlink:href="install-guide#chap-repository"
         xlink:role="http://docbook.org/xlink/role/olink">
    <citetitle>Installing a Repository for Production</citetitle></link>.
  </para>
  <para>
   To configure an individual OpenIDM instance as a part of a clustered
   deployment, follow these steps.
  </para>
  <orderedlist>
   <listitem>
    <para>
     If OpenIDM is running, shut it down using  the OSGi console.
    </para>
    <screen>-&gt; <userinput>shutdown</userinput></screen>
   </listitem>
   <listitem>
    <para>
     Configure OpenIDM for a supported repository, as described in the
     following chapter:
     <link xlink:show="new" xlink:href="install-guide#chap-repository"
           xlink:role="http://docbook.org/xlink/role/olink">
      <citetitle>Installing a Repository for Production</citetitle></link>.
    </para>
    <para>
     Make sure that each database repository file, such as
     <filename>repo.jdbc.json</filename>, points to the appropriate port number
     and IP address for the database.
    </para>
    <para>In that chapter, you should see a reference to a data definition
     language script file. You need to import that file into just one OpenIDM
     instance in your cluster.
    </para>
   </listitem>
   <listitem>
    <para>Follow the steps in this section:
     <xref linkend="cluster-boot-config" /></para>
   </listitem>
   <listitem>
    <para>Follow the steps in this section:
     <xref linkend="cluster-config-file" /></para>
   </listitem>
   <listitem>
    <para>
     If you have scheduled tasks, you should configure persistent schedules
     to start only once across the cluster. For more information, see the
     following section on <link xlink:show="new"
     xlink:href="integrators-guide#persistent-schedules"
     xlink:role="http://docbook.org/xlink/role/olink"><citetitle>Persisted
     Schedules</citetitle></link>.
    </para>
   </listitem>
   <listitem>
    <para>
     Start each instance of OpenIDM.
    </para>
   </listitem>
  </orderedlist>

  <section xml:id="cluster-boot-config">
   <title>Edit the Boot Configuration File</title>

   <para>
    In each OpenIDM instance in your cluster, open the following file:
    <filename>conf/boot/boot.properties</filename>.
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Find the <literal>openidm.node.id</literal> property. Specify a unique
      identifier for each OpenIDM instance. For the first instance, you
      might specify the following:
     </para>
     <screen>openidm.node.id=instance1</screen>
     <para>
      For the second OpenIDM instance, you might specify the following (and so
      on):
     </para>
     <screen>openidm.node.id=instance2</screen>
     <para>
      You can set any value for <literal>openidm.node.id</literal>, as long as
      the value is unique within the cluster. Then the cluster manager can
      detect unavailable OpenIDM instances (by node ID).
     </para>
    </listitem>
    <listitem>
     <para>
      Find the <literal>openidm.instance.type</literal> property.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        On one OpenIDM instance, set <literal>openidm.instance.type</literal>
        as follows:
       </para>
       <programlisting>openidm.instance.type=clustered-first</programlisting>
       <para>
        For a <literal>clustered-first</literal> instance, the Crypto Service
        activates and generates a new secret key (if not present). The Security
        Manager activates and generates a new private key (if not present),
        reloads the keystore within the JVM, and stores the entire keystore in
        the repository, in the following file:
        <filename>security/keystore.jceks</filename>.
       </para>
       <note>
        <para>
         If the <literal>clustered-first</literal> instance of OpenIDM fails,
         you should replace it in the cluster with one of the other
         instances of OpenIDM in the cluster.
        </para>
       </note>
      </listitem>
      <listitem>
       <para>
        On all other OpenIDM instances in the cluster, set
        <literal>openidm.instance.type</literal> as follows:
       </para>
       <programlisting>openidm.instance.type=clustered-additional</programlisting>
       <para>
        For each OpenIDM instance set to <literal>clustered-additional</literal>,
        the Crypto Service activates, but does not generate, a new secret key.
        You will need to create one new secret key which will be added to the
        keystore on each instance. For instructions, see
        <xref linkend="cluster-new-key" />. The Crypto Service does not add any
        <literal>decryptionTransformers</literal>. The Security Manager
        performs the following tasks:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Activates and reads in the keystore from the repository
         </para>
        </listitem>
        <listitem>
         <para>
          Overwrites the local keystore
         </para>
        </listitem>
        <listitem>
         <para>
          Reloads the keystore within the JVM
         </para>
        </listitem>
        <listitem>
         <para>
          Calls the Crypto Service to update the <literal>keySelector</literal>
          with the new keystore. It also adds a
          <literal>decryptionTransformer</literal> to allow the keys to be
          decrypted.
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        If no instance type is specified, the default value for this property
        is <literal>openidm.instance.type=standalone</literal>, which indicates
        that the instance will not be part of a cluster.
       </para>
       <para>
        For a <literal>standalone</literal> instance, the Crypto Service
        activates and generates a new secret key (if not present). The Security
        Manager generates a new private key (if not present) and reloads the
        keystore within the JVM.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
   <para>
    The value of <literal>openidm.instance.type</literal> is used during the
    setup process. When the primary OpenIDM instance has been configured,
    additional nodes are bootstrapped with the security settings
    (keystore and truststore) of the primary node. Once the process is complete,
    all OpenIDM instances in the cluster are considered equal. In other words,
    OpenIDM clusters do not have a "master" node.
   </para>

   <section xml:id="cluster-new-key">
    <title>Creating a Key for Cluster Members</title>

    <para>
     For security, you should set up a new secret key to add to the OpenIDM
     keystore in the following file: <filename>security/keystore.jceks</filename>.
    </para>

    <para>
     You can create the new secret key with the REST commands described in the
     following section:
     <link xlink:show="new" xlink:role="http://docbook.org/xlink/role/olink"
           xlink:href="integrators-guide#security-management-service">
      <citetitle>Accessing the Security Management Service</citetitle></link>.
    </para>

    <para>
     Alternatively, you could set up a "self-signed" key with the following
     <command>keytool</command> command:
    </para>

    <screen>$ <userinput>keytool \
 -genseckey \
 -alias new-sym-key \
 -keyalg AES \
 -keysize 128 \
 -keystore security/keystore.jceks \
 -storetype JCEKS</userinput></screen>

    <para>
     Whether you do so with a <command>keytool</command> command or with a REST
     call, you should now have an <emphasis>alias</emphasis> for your new key.
     Be sure to include the alias at least in the aforementioned
     <literal>clustered-additional</literal> instances. You will need to
     include the new alias <literal>new-sym-key</literal> in both the
     <filename>conf/boot/boot.properties</filename>:
    </para>

    <screen>openidm.config.crypto-alias=new-sym-key</screen>

    <para>
     and the <filename>conf/managed.json</filename> files:
    </para>

    <programlisting language="javascript">{
   "name" : "securityAnswer",
   "encryption" : {
      "key" : "new-sym-key"
   }
   "scope" : "private"
},
{
   "name" : "password",
   "encryption" : {
      "key" : "new-sym-key"
   }
   "scope" : "private"
},</programlisting>

   </section>

  </section>

  <section xml:id="cluster-config-file">
   <title>Edit the Cluster Configuration File</title>
   <para>
    The cluster configuration file is
    <filename>/path/to/openidm/conf/cluster.json</filename>. The default version
    of this file accommodates a cluster, as shown with the value of the
    <literal>enabled</literal> property:
   </para>
   <programlisting language="javascript">{
  "instanceId" : "&amp;{openidm.node.id}",
  "instanceTimeout" : "30000",
  "instanceRecoveryTimeout" : "30000",
  "instanceCheckInInterval" : "5000",
  "instanceCheckInOffset" : "0",
  "enabled" : true
}</programlisting>

   <itemizedlist>
    <listitem>
     <para>
      The <literal>instanceId</literal> is set to the value of
      <literal>openidm.node.id</literal>, as configured in the
      <filename>conf/boot/boot.properties</filename> file. So it is important
      to set unique values for <literal>openidm.node.id</literal> for
      each member of the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>instanceTimeout</literal> specifies the length of time (in
      milliseconds) that a member of the cluster can be "down" before the
      cluster service considers that instance to be in recovery mode.
     </para>
     <para>
      <emphasis>Recovery mode</emphasis> suggests that the
      <literal>instanceTimeout</literal> of an OpenIDM instance has expired,
      and that another OpenIDM instance in the cluster has detected that event.
     </para>
     <para>
      The scheduler component of the second OpenIDM instance should now be
      moving any incomplete jobs into the queue for the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>instanceRecoveryTimeout</literal> specifies the time
      (in milliseconds) that an OpenIDM instance can be in recovery mode before
      it is considered to be offline.
     </para>
     <para>
      This property sets a limit; after this recovery timeout, other members of
      the cluster stops trying access an unavailable OpenIDM instance.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>instanceCheckInInterval</literal> specifies the frequency
      (in milliseconds) that this OpenIDM instance checks in with the cluster
      manager to indicate that it is still online.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>instanceCheckInOffset</literal> specifies an offset (in
      milliseconds) for the checkin timing, when multiple OpenIDM instances
      in a cluster are started simultaneously.
     </para>
     <para>
      The checkin offset prevents multiple OpenIDM instances from checking in
      simultaneously, which would strain the cluster manager resource.
     </para>
    </listitem>
    <listitem>
     <para>
      The <literal>enabled</literal> property notes whether or not the
      clustering service is enabled when you start OpenIDM. Note how this
      property is set to <literal>true</literal> by default.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    If the default cluster configuration is not suitable for your deployment,
    edit the <filename>cluster.json</filename> file for each instance.
   </para>
  </section>
 </section>

 <section xml:id="clustering-scheduled-tasks">
  <title>Managing Scheduled Tasks Across a Cluster</title>
  <itemizedlist>
   <para>
    In a clustered environment, the scheduler service looks for pending jobs and
    handles them as follows:
   </para>
   <listitem>
    <para>
     Non-persistent (in-memory) jobs execute on each node in the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Persistent scheduled jobs are picked up and executed by a single node in
     the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Jobs that are configured as persistent but <emphasis>not concurrent</emphasis>
     run only on one instance in the cluster. That job will not run again at the
     scheduled time, on any instance in the cluster, until the current job is
     complete.
    </para>
    <para>
     For example, a reconciliation operation that runs for longer than the time
     between scheduled intervals will not trigger a duplicate job while it is
     still running.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   OpenIDM instances in a cluster claim jobs in a random order. If one instance
   fails, the cluster manager automatically reassigns unstarted jobs that
   were claimed by that failed instance.
  </para>
  <para>
   For example, if OpenIDM instance A claims a job but does not start it, and
   then loses connectivity, OpenIDM instance B can claim that job.
  </para>
  <para>
   In contrast, if OpenIDM instance A claims a job, starts it, and then loses
   connectivity, other OpenIDM instances in the cluster cannot claim that job.
   That specific job is never completed. Instead, a second OpenIDM instance
   claims the next scheduled occurrence of that job.
  </para>

  <note>
   <para>
    This behavior varies from OpenIDM 2.1.0, in which an unavailable OpenIDM
    instance would have to reconnect to the cluster to free a job
    that it had already claimed.
   </para>
  </note>
  <para>
   If a LiveSync operation leads to multiple changes, a single OpenIDM instance
   process all changes related to that operation.
  </para>

  <section xml:id="revising-cluster-tasks">
   <title>Variations in Scheduled Tasks</title>

   <para>
    Several elements can change the behavior of how scheduled tasks
    operate in a cluster. In this section, you may edit the
    <filename>boot.properties</filename>, <filename>scheduler.json</filename>,
    and <filename>system.properties</filename> files in the
    <filename>conf/</filename> subdirectory.
   </para>

   <section xml:id="revising-cluster-boot">
    <title>Modify an OpenIDM Instance in a Cluster</title>

    <para>
     You can prevent a specific OpenIDM instance from claiming pending jobs, or
     participating in processing clustered schedules. To do so, go to the
     specific OpenIDM instance, open its <filename>boot.properties</filename> file
     and add the following line:
    </para>
    <programlisting>execute.clustered.schedules=false</programlisting>

   </section>

   <section xml:id="revising-cluster-scheduler">
    <title>Modify the OpenIDM Cluster Scheduler</title>

    <para>
     As every OpenIDM instance in a cluster reads its configuration from a
     single repository, token substitution then defines a specific scheduler
     configuration for each instance. It does so by setting
     <literal>"executePersistentSchedules"</literal> to <literal>false</literal>
     in the scheduler configuration file for each instance.
    </para>

    <para>
     The initial scheduler configuration that is loaded into the repository must
     point to the relevant property in <filename>boot.properties</filename>. So,
     open the <filename>scheduler.json</filename> file, and set
     <literal>executePersistentSchedules</literal> as shown:
    </para>
    <programlisting>{
     "executePersistentSchedules" : "&amp;{execute.clustered.schedules}",
}</programlisting>

    <para>
     If the failed instance of OpenIDM did not complete a task, the next
     action depends on the
     <link xlink:show="new" xlink:role="http://docbook.org/xlink/role/olink"
           xlink:href="integrators-guide#scheduler-configuration-file">
      <citetitle>Scheduler Configuration</citetitle></link>, specifically the
     <link xlink:show="new" xlink:role="http://docbook.org/xlink/role/olink"
           xlink:href="integrators-guide#misfire-policy">
      <literal>misfirePolicy</literal></link>.
    </para>
   </section>

   <section xml:id="revising-cluster-overwrite">
    <title>Disable Automating Polling of Configuration Changes</title>

    <para>
     You may not want to allow changes to a configuration file to overwrite the
     global configuration in the repository. To do so, start each instance of
     OpenIDM and then disable automatic polling for configuration changes. To do
     so, open the <filename>system.properties</filename> file and uncomment
     the following line:
    </para>
    <programlisting># openidm.fileinstall.enabled=false</programlisting>
    <para>
     For more information, see the following section:
     <link xlink:show="new"
           xlink:href="integrators-guide#disabling-auto-config-updates"
           xlink:role="http://docbook.org/xlink/role/olink"><citetitle>Disabling
      Automatic Configuration Updates</citetitle></link>.
    </para>

   </section>

  </section>

 </section>

 <section xml:id="cluster-over-REST">
  <title>Managing Nodes Over REST</title>
  <para>
   You can manage clusters and individual nodes over the REST interface, at the
   URL <literal>https://localhost:8443/openidm/cluster/</literal>. The following
   sample REST commands demonstrate the cluster information that is available
   over REST.
  </para>

  <example>
   <title>Displaying the Nodes in the Cluster</title>
   <para>
    The following REST request displays the nodes configured in the cluster, and
    their status.
   </para>
   <screen>$ <userinput>curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster" </userinput>
    <computeroutput>
{
  "results": [
    {
       "state" : "running",
       "instanceId" : "instance2",
       "startup" : "2015-08-28T12:50:37.209-07:00",
       "shutdown" : ""
    },
    {
       "state" : "running",
       "instanceId" : "instance1",
       "startup" : "2015-08-28T11:33:12.650-07:00",
       "shutdown" : ""
    }
  ]</computeroutput>
}  </screen>
  </example>

  <example>
   <title>Checking the State of an Individual Node</title>
   <para>
    To check the status of a specific node, include its node ID in the URL,
    for example:
   </para>
   <screen>$ <userinput> curl \
 --cacert self-signed.crt \
 --header "X-OpenIDM-Username: openidm-admin" \
 --header "X-OpenIDM-Password: openidm-admin" \
 --request GET \
 "https://localhost:8443/openidm/cluster/instance1"</userinput>
    <computeroutput>{
     "state" : "running",
     "instanceId" : "instance1",
     "startup" : "2015-08-28T11:33:12.650-07:00",
     "shutdown" : ""
}</computeroutput></screen>
  </example>
 </section>

</chapter>
